{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "LSTM_Based_Recommender.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2x9KMx9roc5",
        "outputId": "94f3a386-7596-4157-efbd-173c6a4a0c4d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "Q2x9KMx9roc5",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "478a1398"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import re\n",
        "import nltk\n",
        "from operator import itemgetter\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "import plotly.express as px\n",
        "\n",
        "\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "# from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Concatenate, Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Flatten\n",
        "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Convolution1D\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
        "\n",
        "np.random.seed(5)"
      ],
      "id": "478a1398",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "cfd0b282"
      },
      "source": [
        "# # https://arxiv.org/help/api/user-manual\n",
        "# category_map = {\n",
        "# 'cs.CV': 'Computer Vision and Pattern Recognition',\n",
        "# 'cs.LG': 'Machine Learning',\n",
        "# 'cs.RO': 'Robotics',\n",
        "\n",
        "# }"
      ],
      "id": "cfd0b282",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agRl-uk6zrhk"
      },
      "source": [
        "def text2sequence(input_texts , target_texts):\n",
        "  token2index = {\"PAD\":0 , \"SOS\":1 , \"EOS\":2}\n",
        "  token2count = {\"PAD\":1 , \"SOS\":1 , \"EOS\":1}\n",
        "  index2token = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}\n",
        "  num_tokens = 4\n",
        "  input_lines=[]\n",
        "  target_lines=[]\n",
        "\n",
        "  for i in input_texts:\n",
        "    #print(i)\n",
        "    input_lines.append(i.split(\" \"))\n",
        "    # s=eval(i)     \n",
        "    s = i.split(\" \") \n",
        "    for token in s:\n",
        "      if token not in token2index:\n",
        "        # First entry of token into vocabulary\n",
        "        token2index[token] = num_tokens\n",
        "        token2count[token] = 1\n",
        "        index2token[num_tokens] = token\n",
        "        num_tokens += 1\n",
        "      else:\n",
        "        # token exists; increase token count\n",
        "        token2count[token] += 1\n",
        "  \n",
        "  for i in target_texts:\n",
        "    target_lines.append(i.split(\" \"))\n",
        "    # s=eval(i) \n",
        "    s = i.split(\" \")          \n",
        "    for token in s:\n",
        "      if token not in token2index:\n",
        "        # First entry of token into vocabulary\n",
        "        token2index[token] = num_tokens\n",
        "        token2count[token] = 1\n",
        "        index2token[num_tokens] = token\n",
        "        num_tokens += 1\n",
        "      else:\n",
        "        # token exists; increase token count\n",
        "        token2count[token] += 1\n",
        "  \n",
        "  return input_lines , target_lines , token2count"
      ],
      "id": "agRl-uk6zrhk",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DLdhzHJz9YF"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def sequence2tokenization(texts , token2index , sent_length):\n",
        "    indexed_texts =[]\n",
        "    for text in texts:\n",
        "      temp_list=[]     \n",
        "      for token in text:      \n",
        "        if token in token2index.keys():\n",
        "          temp_list.append(token2index[token])\n",
        "        else:\n",
        "          temp_list.append(3)\n",
        "      indexed_texts.append(np.array([1]+temp_list+[2]))\n",
        "      \n",
        "    padded_seq = pad_sequences(indexed_texts,padding='post', maxlen = sent_length)\n",
        "    return padded_seq\n",
        "\n",
        "def encoder_decoder_data(padded_input_texts , padded_target_texts ,num_samples ,num_unique_tokens ):\n",
        "  encoder_input_data = np.zeros((num_samples , 200),dtype='float32')\n",
        "  decoder_input_data = np.zeros((num_samples, 15),dtype='float32')\n",
        "  decoder_target_data = np.zeros((num_samples, 15),dtype='float32')\n",
        "\n",
        "  for i, (input_text, target_text) in enumerate(zip(padded_input_texts, padded_target_texts)):\n",
        "    for t in range(len(input_text)):\n",
        "      encoder_input_data[i, t] = input_text[t]\n",
        "    \n",
        "    for t in range(len(target_text)):\n",
        "      decoder_input_data[i, t] = target_text[t]\n",
        " \n",
        "      if t>0:\n",
        "        decoder_target_data[i, t-1] = target_text[t]\n",
        "  \n",
        "  return encoder_input_data , decoder_input_data , decoder_target_data"
      ],
      "id": "3DLdhzHJz9YF",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTJw9UWLxt21"
      },
      "source": [
        "arxiv_df = pd.read_csv('/content/drive/MyDrive/DATA/pawan/ADL_P1/3cat_ArXiv.csv')\n",
        "\n",
        "# df = arxiv_df[np.logical_or((arxiv_df['category'].values == 'Machine Learning'),\n",
        "#                            (arxiv_df['category'].values == 'Computer Vision and Pattern Recognition'))]\n",
        "\n",
        "# using loc() method\n",
        "mask = arxiv_df['category'].values == 'Computer Vision and Pattern Recognition'\n",
        "df = arxiv_df.loc[mask]\n",
        "\n",
        "# from sklearn.utils import shuffle\n",
        "# df = shuffle(df)\n",
        "\n"
      ],
      "id": "FTJw9UWLxt21",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "qleHbxEH0hbx",
        "outputId": "1c3e41ee-a173-4a1c-a5c9-8c307b02e985"
      },
      "source": [
        "df.head()"
      ],
      "id": "qleHbxEH0hbx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>category</th>\n",
              "      <th>clean_title</th>\n",
              "      <th>clean_abstract</th>\n",
              "      <th>soup</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Text Line Segmentation of Historical Documents...</td>\n",
              "      <td>There is a huge amount of historical documen...</td>\n",
              "      <td>Computer Vision and Pattern Recognition</td>\n",
              "      <td>text line segmentation historical documents su...</td>\n",
              "      <td>huge amount historical documents libraries var...</td>\n",
              "      <td>text line segmentation historical documents su...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>Rough Sets Computations to Impute Missing Data</td>\n",
              "      <td>Many techniques for handling missing data ha...</td>\n",
              "      <td>Computer Vision and Pattern Recognition</td>\n",
              "      <td>rough sets computations impute missing data</td>\n",
              "      <td>many techniques handling missing data proposed...</td>\n",
              "      <td>rough sets computations impute missing data, r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>Riemannian level-set methods for tensor-valued...</td>\n",
              "      <td>We present a novel approach for the derivati...</td>\n",
              "      <td>Computer Vision and Pattern Recognition</td>\n",
              "      <td>riemannian level set methods tensor valued data</td>\n",
              "      <td>present novel approach derivation pde modeling...</td>\n",
              "      <td>riemannian level set methods tensor valued dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>Multiresolution Approximation of Polygonal Cur...</td>\n",
              "      <td>We propose a new algorithm to the problem of...</td>\n",
              "      <td>Computer Vision and Pattern Recognition</td>\n",
              "      <td>multiresolution approximation polygonal curves...</td>\n",
              "      <td>propose new algorithm problem polygonal curve ...</td>\n",
              "      <td>multiresolution approximation polygonal curves...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>Medical Image Segmentation and Localization us...</td>\n",
              "      <td>This paper presents deformable templates as ...</td>\n",
              "      <td>Computer Vision and Pattern Recognition</td>\n",
              "      <td>medical image segmentation localization using ...</td>\n",
              "      <td>paper presents deformable templates tool segme...</td>\n",
              "      <td>medical image segmentation localization using ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Unnamed: 0  ...                                               soup\n",
              "2            2  ...  text line segmentation historical documents su...\n",
              "8            8  ...  rough sets computations impute missing data, r...\n",
              "9            9  ...  riemannian level set methods tensor valued dat...\n",
              "10          10  ...  multiresolution approximation polygonal curves...\n",
              "12          12  ...  medical image segmentation localization using ...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNdh70HrtUhs"
      },
      "source": [
        "# for i in df['title'][4000:8000]:\n",
        "#   print(i)"
      ],
      "id": "BNdh70HrtUhs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwWG4IL61FD2",
        "outputId": "1b091129-9e1a-4b22-d888-a1f152182cf1"
      },
      "source": [
        "df.size"
      ],
      "id": "rwWG4IL61FD2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "337904"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18xncM3S0ScG"
      },
      "source": [
        "encoder_input = df['abstract'][4000:8000]\n",
        "decoder_input = df['title'][4000:8000]\n"
      ],
      "id": "18xncM3S0ScG",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60GeFDfj0KTg"
      },
      "source": [
        "input_lines , target_lines , token2count = text2sequence(encoder_input , decoder_input)\n",
        "k = 100000\n",
        "encoder_sent_length=200\n",
        "decoder_sent_length = 15\n",
        "num_unique_tokens = 100000 + 4\n",
        "num_samples = len(input_lines)\n",
        "\n",
        "token_with_count = dict(sorted(token2count.items(), key = itemgetter(1), reverse = True)[:k])\n",
        "token2index = {\"PAD\":0, \"SOS\":1, \"EOS\":2, \"OOV\":3}\n",
        "index2token = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\", 3: \"OOV\"}\n",
        "num_count = 4\n",
        "for i in token_with_count:\n",
        "  token2index[i] = num_count\n",
        "  index2token[num_count] = i\n",
        "  num_count += 1\n"
      ],
      "id": "60GeFDfj0KTg",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVVn0gcz2rfN",
        "outputId": "9356f46b-ad60-4377-bc6d-deda783e7efb"
      },
      "source": [
        "padded_input_texts = sequence2tokenization(input_lines , token2index , encoder_sent_length)\n",
        "padded_target_texts = sequence2tokenization(target_lines , token2index , decoder_sent_length)\n",
        "encoder_input_data , decoder_input_data , decoder_target_data = encoder_decoder_data(padded_input_texts , padded_target_texts ,num_samples ,num_unique_tokens )\n",
        "print(encoder_input_data.shape)\n",
        "print(decoder_input_data.shape)"
      ],
      "id": "aVVn0gcz2rfN",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4000, 200)\n",
            "(4000, 15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KH8AE1_7hVV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b8d4fca-9d7e-4278-b9ea-f49fe78dc317"
      },
      "source": [
        "\n",
        "latent_dim=512\n",
        "batch_size=64\n",
        "epochs = 10\n",
        "emb_size = 512\n",
        "# Define an input sequence and process it.\n",
        "\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding=  Embedding(num_unique_tokens, emb_size , mask_zero = True )(encoder_inputs)\n",
        "# custom_embeding_matrix = np.array(encoder_embedding.get_weights())[0]\n",
        "# custom_embeding_matrix.shape\n",
        "encoder_lstm = Bidirectional(LSTM(latent_dim, return_state=True))\n",
        "encoder_outputs, forward_h, forward_c, backward_h, backward_c =encoder_lstm(encoder_embedding)\n",
        "\n",
        "state_h = Concatenate()([forward_h, backward_h])\n",
        "state_c = Concatenate()([forward_c, backward_c])\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding=  Embedding(num_unique_tokens, emb_size , mask_zero = True)\n",
        "x= decoder_embedding(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(x,initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_unique_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
        "model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['acc'])"
      ],
      "id": "-KH8AE1_7hVV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x7f244efb08c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 546, in __del__\n",
            "    handle=self._handle, deleter=self._deleter)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1264, in delete_iterator\n",
            "    _ctx, \"DeleteIterator\", name, handle, deleter)\n",
            "KeyboardInterrupt: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdvG5wNh9Vjv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c86c60b1-d747-416f-c237-581a1271498c"
      },
      "source": [
        "model.summary()"
      ],
      "id": "FdvG5wNh9Vjv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, None, 512)    51202048    input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) [(None, 1024), (None 4198400     embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, None, 512)    51202048    input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 1024)         0           bidirectional_2[0][1]            \n",
            "                                                                 bidirectional_2[0][3]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 1024)         0           bidirectional_2[0][2]            \n",
            "                                                                 bidirectional_2[0][4]            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   [(None, None, 1024), 6295552     embedding_5[0][0]                \n",
            "                                                                 concatenate_4[0][0]              \n",
            "                                                                 concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, None, 100004) 102504100   lstm_5[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 215,402,148\n",
            "Trainable params: 215,402,148\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZtgsSwL7hRf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dfb35d4-6722-48a0-cca0-053268f905f0"
      },
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', mode = 'min' ,patience=4)\n",
        "history1= model.fit(\n",
        "    [encoder_input_data, decoder_input_data],decoder_target_data,\n",
        "    batch_size=64,\n",
        "    epochs=100,\n",
        "    callbacks=[callback]\n",
        ")"
      ],
      "id": "JZtgsSwL7hRf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "63/63 [==============================] - 63s 783ms/step - loss: 5.7464 - acc: 0.1044\n",
            "Epoch 2/100\n",
            "63/63 [==============================] - 50s 788ms/step - loss: 4.6315 - acc: 0.1693\n",
            "Epoch 3/100\n",
            "63/63 [==============================] - 50s 788ms/step - loss: 4.2336 - acc: 0.2090\n",
            "Epoch 4/100\n",
            "63/63 [==============================] - 50s 789ms/step - loss: 4.0314 - acc: 0.2293\n",
            "Epoch 5/100\n",
            "63/63 [==============================] - 50s 791ms/step - loss: 3.8557 - acc: 0.2556\n",
            "Epoch 6/100\n",
            "63/63 [==============================] - 50s 789ms/step - loss: 3.6953 - acc: 0.2777\n",
            "Epoch 7/100\n",
            "63/63 [==============================] - 50s 788ms/step - loss: 3.5447 - acc: 0.2929\n",
            "Epoch 8/100\n",
            "63/63 [==============================] - 50s 789ms/step - loss: 3.3951 - acc: 0.3053\n",
            "Epoch 9/100\n",
            "63/63 [==============================] - 50s 788ms/step - loss: 3.2473 - acc: 0.3166\n",
            "Epoch 10/100\n",
            "63/63 [==============================] - 50s 789ms/step - loss: 3.0969 - acc: 0.3291\n",
            "Epoch 11/100\n",
            "63/63 [==============================] - 50s 786ms/step - loss: 2.9462 - acc: 0.3426\n",
            "Epoch 12/100\n",
            "63/63 [==============================] - 50s 787ms/step - loss: 2.7976 - acc: 0.3564\n",
            "Epoch 13/100\n",
            "63/63 [==============================] - 50s 788ms/step - loss: 2.6437 - acc: 0.3762\n",
            "Epoch 14/100\n",
            "63/63 [==============================] - 49s 784ms/step - loss: 2.4869 - acc: 0.4000\n",
            "Epoch 15/100\n",
            "63/63 [==============================] - 50s 786ms/step - loss: 2.3377 - acc: 0.4300\n",
            "Epoch 16/100\n",
            "63/63 [==============================] - 50s 786ms/step - loss: 2.1918 - acc: 0.4584\n",
            "Epoch 17/100\n",
            "63/63 [==============================] - 50s 787ms/step - loss: 2.0461 - acc: 0.4949\n",
            "Epoch 18/100\n",
            "63/63 [==============================] - 49s 786ms/step - loss: 1.9106 - acc: 0.5278\n",
            "Epoch 19/100\n",
            "63/63 [==============================] - 50s 787ms/step - loss: 1.7808 - acc: 0.5603\n",
            "Epoch 20/100\n",
            "63/63 [==============================] - 49s 786ms/step - loss: 1.6592 - acc: 0.5912\n",
            "Epoch 21/100\n",
            "63/63 [==============================] - 49s 785ms/step - loss: 1.5440 - acc: 0.6206\n",
            "Epoch 22/100\n",
            "63/63 [==============================] - 49s 785ms/step - loss: 1.4342 - acc: 0.6474\n",
            "Epoch 23/100\n",
            "63/63 [==============================] - 50s 787ms/step - loss: 1.3305 - acc: 0.6772\n",
            "Epoch 24/100\n",
            "63/63 [==============================] - 50s 786ms/step - loss: 1.2399 - acc: 0.7001\n",
            "Epoch 25/100\n",
            "63/63 [==============================] - 49s 785ms/step - loss: 1.1605 - acc: 0.7201\n",
            "Epoch 26/100\n",
            "63/63 [==============================] - 50s 786ms/step - loss: 1.0795 - acc: 0.7433\n",
            "Epoch 27/100\n",
            "63/63 [==============================] - 50s 786ms/step - loss: 1.0001 - acc: 0.7636\n",
            "Epoch 28/100\n",
            "63/63 [==============================] - 50s 790ms/step - loss: 0.9325 - acc: 0.7815\n",
            "Epoch 29/100\n",
            "63/63 [==============================] - 49s 783ms/step - loss: 0.8727 - acc: 0.7955\n",
            "Epoch 30/100\n",
            "63/63 [==============================] - 49s 784ms/step - loss: 0.8128 - acc: 0.8114\n",
            "Epoch 31/100\n",
            "63/63 [==============================] - 49s 785ms/step - loss: 0.7575 - acc: 0.8265\n",
            "Epoch 32/100\n",
            "63/63 [==============================] - 49s 784ms/step - loss: 0.7048 - acc: 0.8385\n",
            "Epoch 33/100\n",
            "63/63 [==============================] - 50s 786ms/step - loss: 0.6551 - acc: 0.8526\n",
            "Epoch 34/100\n",
            "63/63 [==============================] - 49s 786ms/step - loss: 0.6104 - acc: 0.8626\n",
            "Epoch 35/100\n",
            "63/63 [==============================] - 49s 785ms/step - loss: 0.5672 - acc: 0.8731\n",
            "Epoch 36/100\n",
            "63/63 [==============================] - 49s 784ms/step - loss: 0.5254 - acc: 0.8837\n",
            "Epoch 37/100\n",
            "63/63 [==============================] - 49s 785ms/step - loss: 0.4878 - acc: 0.8918\n",
            "Epoch 38/100\n",
            "63/63 [==============================] - 49s 785ms/step - loss: 0.4512 - acc: 0.9003\n",
            "Epoch 39/100\n",
            "63/63 [==============================] - 49s 785ms/step - loss: 0.4152 - acc: 0.9090\n",
            "Epoch 40/100\n",
            "63/63 [==============================] - 49s 783ms/step - loss: 0.3812 - acc: 0.9168\n",
            "Epoch 41/100\n",
            "63/63 [==============================] - 49s 784ms/step - loss: 0.3496 - acc: 0.9252\n",
            "Epoch 42/100\n",
            "63/63 [==============================] - 49s 784ms/step - loss: 0.3198 - acc: 0.9331\n",
            "Epoch 43/100\n",
            "63/63 [==============================] - 49s 784ms/step - loss: 0.2921 - acc: 0.9382\n",
            "Epoch 44/100\n",
            "63/63 [==============================] - 49s 783ms/step - loss: 0.2656 - acc: 0.9452\n",
            "Epoch 45/100\n",
            "63/63 [==============================] - 49s 783ms/step - loss: 0.2392 - acc: 0.9525\n",
            "Epoch 46/100\n",
            "63/63 [==============================] - 49s 783ms/step - loss: 0.2157 - acc: 0.9584\n",
            "Epoch 47/100\n",
            "63/63 [==============================] - 49s 783ms/step - loss: 0.1922 - acc: 0.9649\n",
            "Epoch 48/100\n",
            "63/63 [==============================] - 49s 785ms/step - loss: 0.1712 - acc: 0.9704\n",
            "Epoch 49/100\n",
            "63/63 [==============================] - 49s 784ms/step - loss: 0.1519 - acc: 0.9753\n",
            "Epoch 50/100\n",
            "63/63 [==============================] - 49s 785ms/step - loss: 0.1342 - acc: 0.9797\n",
            "Epoch 51/100\n",
            "63/63 [==============================] - 49s 785ms/step - loss: 0.1187 - acc: 0.9842\n",
            "Epoch 52/100\n",
            "63/63 [==============================] - 49s 783ms/step - loss: 0.1060 - acc: 0.9866\n",
            "Epoch 53/100\n",
            "63/63 [==============================] - 49s 785ms/step - loss: 0.0924 - acc: 0.9896\n",
            "Epoch 54/100\n",
            "63/63 [==============================] - 49s 783ms/step - loss: 0.0812 - acc: 0.9920\n",
            "Epoch 55/100\n",
            "63/63 [==============================] - 49s 783ms/step - loss: 0.0706 - acc: 0.9937\n",
            "Epoch 56/100\n",
            "63/63 [==============================] - 49s 782ms/step - loss: 0.0604 - acc: 0.9954\n",
            "Epoch 57/100\n",
            "63/63 [==============================] - 49s 784ms/step - loss: 0.0529 - acc: 0.9967\n",
            "Epoch 58/100\n",
            "63/63 [==============================] - 49s 784ms/step - loss: 0.0465 - acc: 0.9975\n",
            "Epoch 59/100\n",
            "63/63 [==============================] - 49s 783ms/step - loss: 0.0403 - acc: 0.9983\n",
            "Epoch 60/100\n",
            "63/63 [==============================] - 49s 782ms/step - loss: 0.0355 - acc: 0.9989\n",
            "Epoch 61/100\n",
            "63/63 [==============================] - 49s 784ms/step - loss: 0.0313 - acc: 0.9993\n",
            "Epoch 62/100\n",
            "63/63 [==============================] - 49s 782ms/step - loss: 0.0278 - acc: 0.9993\n",
            "Epoch 63/100\n",
            "63/63 [==============================] - 49s 785ms/step - loss: 0.0250 - acc: 0.9995\n",
            "Epoch 64/100\n",
            "63/63 [==============================] - 49s 783ms/step - loss: 0.0227 - acc: 0.9996\n",
            "Epoch 65/100\n",
            "63/63 [==============================] - 50s 787ms/step - loss: 0.0208 - acc: 0.9996\n",
            "Epoch 66/100\n",
            "63/63 [==============================] - 49s 783ms/step - loss: 0.0189 - acc: 0.9997\n",
            "Epoch 67/100\n",
            "63/63 [==============================] - 49s 784ms/step - loss: 0.0172 - acc: 0.9997\n",
            "Epoch 68/100\n",
            "63/63 [==============================] - 49s 784ms/step - loss: 0.0159 - acc: 0.9998\n",
            "Epoch 69/100\n",
            "63/63 [==============================] - 49s 782ms/step - loss: 0.0148 - acc: 0.9997\n",
            "Epoch 70/100\n",
            "63/63 [==============================] - 49s 783ms/step - loss: 0.0139 - acc: 0.9997\n",
            "Epoch 71/100\n",
            "63/63 [==============================] - 49s 783ms/step - loss: 0.0132 - acc: 0.9997\n",
            "Epoch 72/100\n",
            "63/63 [==============================] - 49s 783ms/step - loss: 0.0123 - acc: 0.9996\n",
            "Epoch 73/100\n",
            "63/63 [==============================] - 49s 783ms/step - loss: 0.0116 - acc: 0.9997\n",
            "Epoch 74/100\n",
            "63/63 [==============================] - 49s 784ms/step - loss: 0.0109 - acc: 0.9998\n",
            "Epoch 75/100\n",
            "63/63 [==============================] - 49s 783ms/step - loss: 0.0102 - acc: 0.9998\n",
            "Epoch 76/100\n",
            "63/63 [==============================] - 49s 782ms/step - loss: 0.0096 - acc: 0.9998\n",
            "Epoch 77/100\n",
            "63/63 [==============================] - 49s 782ms/step - loss: 0.0092 - acc: 0.9998\n",
            "Epoch 78/100\n",
            "63/63 [==============================] - 49s 783ms/step - loss: 0.0086 - acc: 0.9998\n",
            "Epoch 79/100\n",
            "63/63 [==============================] - 49s 782ms/step - loss: 0.0083 - acc: 0.9998\n",
            "Epoch 80/100\n",
            "63/63 [==============================] - 49s 784ms/step - loss: 0.0076 - acc: 0.9998\n",
            "Epoch 81/100\n",
            "63/63 [==============================] - 50s 786ms/step - loss: 0.0072 - acc: 0.9998\n",
            "Epoch 82/100\n",
            "63/63 [==============================] - 49s 782ms/step - loss: 0.0070 - acc: 0.9998\n",
            "Epoch 83/100\n",
            "63/63 [==============================] - 49s 785ms/step - loss: 0.0067 - acc: 0.9998\n",
            "Epoch 84/100\n",
            "63/63 [==============================] - 49s 783ms/step - loss: 0.0064 - acc: 0.9998\n",
            "Epoch 85/100\n",
            "63/63 [==============================] - 49s 781ms/step - loss: 0.0061 - acc: 0.9998\n",
            "Epoch 86/100\n",
            "63/63 [==============================] - 49s 781ms/step - loss: 0.0059 - acc: 0.9998\n",
            "Epoch 87/100\n",
            "63/63 [==============================] - 49s 783ms/step - loss: 0.0057 - acc: 0.9998\n",
            "Epoch 88/100\n",
            "63/63 [==============================] - 49s 785ms/step - loss: 0.0421 - acc: 0.9915\n",
            "Epoch 89/100\n",
            "63/63 [==============================] - 49s 784ms/step - loss: 0.2450 - acc: 0.9246\n",
            "Epoch 90/100\n",
            "63/63 [==============================] - 49s 785ms/step - loss: 0.2047 - acc: 0.9353\n",
            "Epoch 91/100\n",
            "63/63 [==============================] - 50s 787ms/step - loss: 0.0797 - acc: 0.9820\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "ee5AKIgvx-BB",
        "outputId": "ad5075d3-6291-4375-cad1-dd4a0293d4ac"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.title('Training Loss Vs Epochs ' )\n",
        "# plt.plot(history1.history['acc'],label='train_set_acc')\n",
        "plt.plot(history1.history['loss'],label='train_set_loss')\n",
        "plt.legend()\n",
        "plt.show() "
      ],
      "id": "ee5AKIgvx-BB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "DJbYWhhyX4Lo",
        "outputId": "848bf5a4-57be-4fcc-f688-5064bee666a0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.title('Training Loss Vs Epochs ' )\n",
        "plt.plot(history1.history['acc'],label='train_set_acc')\n",
        "# plt.plot(history1.history['loss'],label='train_set_loss')\n",
        "plt.legend()\n",
        "plt.show() "
      ],
      "id": "DJbYWhhyX4Lo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EHsBQ8Gj22c"
      },
      "source": [
        "# tf.keras.models.save_model(model , \"/content/drive/MyDrive/DATA/pawan/ADL_P1/LSTM_rec\")"
      ],
      "id": "_EHsBQ8Gj22c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hMR3sySPOOf"
      },
      "source": [
        "model = tf.keras.models.load_model(\"/content/drive/MyDrive/DATA/pawan/ADL_P1/LSTM_rec\")"
      ],
      "id": "6hMR3sySPOOf",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-pqUCIs-S-d"
      },
      "source": [
        "import nltk\n",
        "\n",
        "latent_dim=512\n",
        "encoder_inputs = model.input[0]\n",
        "sent_emb1 = model.layers[1].output\n",
        "\n",
        "encoder_model = Model(encoder_inputs, sent_emb1)\n"
      ],
      "id": "o-pqUCIs-S-d",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxSLtLE41aKZ"
      },
      "source": [
        "sent_emb = []\n",
        "def decode_sequence(input_seq):\n",
        "    temp = encoder_model.predict(input_seq)\n",
        "    sent_emb.append(temp[0])\n",
        "\n",
        "for seq_index in range(len(encoder_input_data)):\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    if(seq_index%1000 == 0):\n",
        "      print(seq_index)\n",
        "    decode_sequence(input_seq)\n",
        "       "
      ],
      "id": "SxSLtLE41aKZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuTc_qUxOvkE"
      },
      "source": [
        "# np.save(\"/content/drive/MyDrive/DATA/pawan/ADL_P1/temp_LSTM_sent_embedding\", sent_emb)"
      ],
      "id": "uuTc_qUxOvkE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijrF2MTvRkRH"
      },
      "source": [
        "sent_emb = np.load(\"/content/drive/MyDrive/DATA/pawan/ADL_P1/temp_LSTM_sent_embedding.npy\",  allow_pickle=True)"
      ],
      "id": "ijrF2MTvRkRH",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16xoagi7Kq8X"
      },
      "source": [
        "sentence_embeddings = []\n",
        "i = 0\n",
        "while(i < 4010):\n",
        "  sentence_embeddings.append(np.asarray(sent_emb[i]).reshape(-1))\n",
        "  i = i + 1\n"
      ],
      "id": "16xoagi7Kq8X",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOelkP7YL40x",
        "outputId": "71afdec7-d830-48e7-bb9b-8678c9ed8578"
      },
      "source": [
        "np.asarray(sentence_embeddings).shape"
      ],
      "id": "LOelkP7YL40x",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4010, 102400)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNPYECSdBA_F"
      },
      "source": [
        "def get_query_emb(input_seq):\n",
        "    temp = encoder_model.predict(input_seq)\n",
        "    return temp[0]"
      ],
      "id": "YNPYECSdBA_F",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ThD39wRSF24"
      },
      "source": [
        "from operator import itemgetter\n",
        "import scipy \n",
        "def get_recommendation(seq_index):\n",
        "  queries = encoder_input_data[seq_index: seq_index + 1]\n",
        "  query_embeddings = get_query_emb(queries)\n",
        "  query_embeddings = query_embeddings.reshape(-1)\n",
        "  new_emb = []\n",
        "  new_emb.append(query_embeddings)\n",
        "  number_top_matches = 10\n",
        "  for query, query_embedding in zip(queries, new_emb):\n",
        "      distances = scipy.spatial.distance.cdist(new_emb, sentence_embeddings, \"cosine\")[0]\n",
        "      results = zip(range(len(distances)), distances)\n",
        "      results = sorted(results, key=lambda x: x[1])\n",
        "\n",
        "      print(\"\\n\\n======================\\n\\n\")\n",
        "      list = target_lines[seq_index]\n",
        "      str1 = ' '.join(list)\n",
        "      print( \"Query : \",str1 )\n",
        "      # print(\"\\nTop 10 most similar sentences in corpus:\")\n",
        "      temp = []\n",
        "      # temp2 = {}\n",
        "      for idx, distance in results[0:number_top_matches]:\n",
        "        list = target_lines[idx]\n",
        "        str1 = ' '.join(list)\n",
        "        # temp2[1-distance] = str1\n",
        "        temp.append(str1)\n",
        "      # temp = sorted(temp2.items(), key = itemgetter(0), reverse=True)\n",
        "      return temp"
      ],
      "id": "3ThD39wRSF24",
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jc5BzvSf_Ei-",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ed46b02-6f82-474c-d508-2861d09165b0"
      },
      "source": [
        "seq_index = 1136\n",
        "get_recommendation(seq_index)\n"
      ],
      "id": "Jc5BzvSf_Ei-",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query :  A Neural Approach to Blind Motion Deblurring\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A Neural Approach to Blind Motion Deblurring',\n",
              " 'Abnormal Event Detection in Videos using Spatiotemporal Autoencoder',\n",
              " 'AdaScan: Adaptive Scan Pooling in Deep Convolutional Neural Networks for\\n  Human Action Recognition in Videos',\n",
              " 'Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image\\n  Segmentation',\n",
              " 'Hashmod: A Hashing Method for Scalable 3D Object Detection',\n",
              " 'Enhanced Object Detection via Fusion With Prior Beliefs from Image\\n  Classification',\n",
              " 'Video Interpolation using Optical Flow and Laplacian Smoothness',\n",
              " 'The More You Know: Using Knowledge Graphs for Image Classification',\n",
              " 'A reliable order-statistics-based approximate nearest neighbor search\\n  algorithm',\n",
              " 'Human Pose Estimation in Space and Time using 3D CNN']"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdpaxzxH-R6m",
        "outputId": "ccadd732-b081-4130-c32c-4fb3508689eb"
      },
      "source": [
        "seq_index = 3652\n",
        "get_recommendation(seq_index)"
      ],
      "id": "BdpaxzxH-R6m",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query :  Deep Convolutional Poses for Human Interaction Recognition in Monocular\n",
            "  Videos\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Deep Convolutional Poses for Human Interaction Recognition in Monocular\\n  Videos',\n",
              " 'Graph Construction with Label Information for Semi-Supervised Learning',\n",
              " 'Approaching the Computational Color Constancy as a Classification\\n  Problem through Deep Learning',\n",
              " 'Deep convolutional neural networks for pedestrian detection',\n",
              " 'The Role of Typicality in Object Classification: Improving The\\n  Generalization Capacity of Convolutional Neural Networks',\n",
              " 'Towards Better Analysis of Deep Convolutional Neural Networks',\n",
              " 'A Data-driven Approach for Human Pose Tracking Based on Spatio-temporal\\n  Pictorial Structure',\n",
              " 'Fast and Provably Accurate Bilateral Filtering',\n",
              " 'Texture Synthesis with Spatial Generative Adversarial Networks',\n",
              " 'Detecting events and key actors in multi-person videos']"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jlrdd9SR8xQm",
        "outputId": "91a0a80e-9962-4350-d713-88e90d63726a"
      },
      "source": [
        "seq_index = 2088\n",
        "get_recommendation(seq_index)"
      ],
      "id": "Jlrdd9SR8xQm",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query :  Persistent Homology on Grassmann Manifolds for Analysis of Hyperspectral\n",
            "  Movies\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Persistent Homology on Grassmann Manifolds for Analysis of Hyperspectral\\n  Movies',\n",
              " 'Supervised multiview learning based on simultaneous learning of\\n  multiview intact and single view classifier',\n",
              " 'Where to Focus: Query Adaptive Matching for Instance Retrieval Using\\n  Convolutional Feature Maps',\n",
              " 'Efficient Discriminative Nonorthogonal Binary Subspace with its\\n  Application to Visual Tracking',\n",
              " 'Novel Views of Objects from a Single Image',\n",
              " 'Dual Principal Component Pursuit',\n",
              " 'Pandora: Description of a Painting Database for Art Movement Recognition\\n  with Baselines and Perspectives',\n",
              " 'Laplacian regularized low rank subspace clustering',\n",
              " 'Comprehensive Feature-based Robust Video Fingerprinting Using Tensor\\n  Model',\n",
              " 'A Dataset for Improved RGBD-based Object Detection and Pose Estimation\\n  for Warehouse Pick-and-Place']"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1dVELX3gEBN",
        "outputId": "f8d6774b-cfdd-4ce5-8f13-f36dae5cd804"
      },
      "source": [
        "seq_index = 3848\n",
        "get_recommendation(seq_index)"
      ],
      "id": "J1dVELX3gEBN",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query :  Learning Quadrupedal Locomotion over Challenging Terrain\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Learning Quadrupedal Locomotion over Challenging Terrain',\n",
              " 'A Concave Optimization Algorithm for Matching Partially Overlapping\\n  Point Sets',\n",
              " 'Statistics of RGBD Images',\n",
              " 'Automatic Segmentation of Dynamic Objects from an Image Pair',\n",
              " 'Deep convolutional neural networks for pedestrian detection',\n",
              " 'Image denoising via group sparsity residual constraint',\n",
              " 'Predicting the Category and Attributes of Visual Search Targets Using\\n  Deep Gaze Pooling',\n",
              " 'Greedy Structure Learning of Hierarchical Compositional Models',\n",
              " 'A Data-driven Approach for Human Pose Tracking Based on Spatio-temporal\\n  Pictorial Structure',\n",
              " 'Predicting Face Recognition Performance Using Image Quality']"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIuDY1NH9niG",
        "outputId": "b9e7c6b1-2ae4-4fa5-b47d-91b176520e4e"
      },
      "source": [
        "seq_index = 1038\n",
        "get_recommendation(seq_index)"
      ],
      "id": "OIuDY1NH9niG",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query :  US-Cut: Interactive Algorithm for rapid Detection and Segmentation of\n",
            "  Liver Tumors in Ultrasound Acquisitions\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['US-Cut: Interactive Algorithm for rapid Detection and Segmentation of\\n  Liver Tumors in Ultrasound Acquisitions',\n",
              " 'Novel Views of Objects from a Single Image',\n",
              " 'Adaptive Algorithm and Platform Selection for Visual Detection and\\n  Tracking',\n",
              " 'Hierarchical Deep Learning Architecture For 10K Objects Classification',\n",
              " 'Human Action Recognition using Factorized Spatio-Temporal Convolutional\\n  Networks',\n",
              " 'Saliency Driven Object recognition in egocentric videos with deep CNN',\n",
              " 'Where to Focus: Query Adaptive Matching for Instance Retrieval Using\\n  Convolutional Feature Maps',\n",
              " 'Using Filter Banks in Convolutional Neural Networks for Texture\\n  Classification',\n",
              " 'UnrealStereo: Controlling Hazardous Factors to Analyze Stereo Vision',\n",
              " 'Sketch-based Image Retrieval from Millions of Images under Rotation,\\n  Translation and Scale Variations']"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7W7jUzYi-S1M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "45fb87ec-473f-4388-9805-3d04c94d1dc7"
      },
      "source": [
        "import nltk\n",
        "BleuScore = 0\n",
        "sent_length = 200\n",
        "num_sample =  len(encoder_input_data)\n",
        "sentence_emb = []\n",
        "def decode_sequence(input_seq):\n",
        "    \n",
        "    abstract_emb = encoder_model.predict(input_seq)\n",
        "    temp = []\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    \n",
        "    target_seq[0, 0] = 1\n",
        "    \n",
        "    stop_condition = False\n",
        "    decoded_sentence = []\n",
        "\n",
        "    while not stop_condition:\n",
        "\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        temp.append(c)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        #print(\"sampled_token_index\",sampled_token_index)\n",
        "        sampled_char = index2token[sampled_token_index]\n",
        "        #print(\"sampled_char\",sampled_char)\n",
        "        \n",
        "\n",
        "        # # Exit condition: either hit max length\n",
        "        # # or find stop character.\n",
        "        \n",
        "        if sampled_char == 'EOS' or len(decoded_sentence) > sent_length:\n",
        "          stop_condition = True\n",
        "        else:\n",
        "          decoded_sentence.append(sampled_char)\n",
        "        \n",
        "\n",
        "\n",
        "        # # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        states_value = [h, c]\n",
        "    sentence_emb.append(temp)\n",
        "    return decoded_sentence\n",
        "\n",
        "for seq_index in range(2):\n",
        "    input_seq = encoder_input_data[2008+seq_index: 2008+seq_index + 1]\n",
        "    if(seq_index%1000 == 0):\n",
        "      print(seq_index)\n",
        "    # print('Input sentence length : ', encoder_input_data[seq_index].size)\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "  \n",
        "    # print('-',seq_index)\n",
        "    # print('Input sentence:      ', input_valid_lines[seq_index].split(' '))\n",
        "    print('Decoded sentence:    ', decoded_sentence)\n"
      ],
      "id": "7W7jUzYi-S1M",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-38d2e222c16b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# print('Input sentence length : ', encoder_input_data[seq_index].size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mdecoded_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# print('-',seq_index)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-38d2e222c16b>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstop_condition\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moutput_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstates_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'decoder_model' is not defined"
          ]
        }
      ]
    }
  ]
}